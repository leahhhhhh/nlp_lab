{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCHPkKbuhPF6"
   },
   "source": [
    "# Lab 03\n",
    "\n",
    "Today we will investigate some word representation models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZ0HLTPGemjV"
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GNyhgK5QTOuD",
    "outputId": "2ecb0b51-3667-4e5e-9082-2ed258b31c07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jiawen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import re\n",
    "\n",
    "# For parsing our XML data\n",
    "from lxml import etree \n",
    "\n",
    "# For data processing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# For implementing the word2vec family of algorithms\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bmae7urS8RHD"
   },
   "source": [
    "### Download data from Google Drive\n",
    "For today's lab we will download and use TED script data we are providing. The data is stored in Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV7vMHSahdnf"
   },
   "source": [
    "#### Option (1) Colab\n",
    "\n",
    "**Google Drive Access Setup**\n",
    "\n",
    "Running the following code should direct you to the Google Sign In page. Sign in with your own Google account by following the instructions on the page.\n",
    "\n",
    "After that, the code should download the file.\n",
    "\n",
    "**Downloading TED Scripts from Google Drive**\n",
    "\n",
    "Click on left side \"Files\" tab to check if the file has downloaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oTSQtnPkfyzj"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydrive\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauth\u001b[39;00m \u001b[39mimport\u001b[39;00m GoogleAuth\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydrive\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdrive\u001b[39;00m \u001b[39mimport\u001b[39;00m GoogleDrive\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m auth\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moauth2client\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m GoogleCredentials\n\u001b[1;32m      6\u001b[0m \u001b[39m# Authenticate and create the PyDrive client.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "id = '17tGzZyLbz1W3xedRhhl-j5i1ndgaM_yM'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('ted_en-20160408.xml')\n",
    "\n",
    "id = '1709bhW6wcZx9jnypRFNPgnY-P51OEfIJ'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('ted_en-20160408.json')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewAbjQzThnT5"
   },
   "source": [
    "#### Option (2) Local\n",
    "\n",
    "Download these two files:\n",
    "- https://drive.google.com/file/d/17tGzZyLbz1W3xedRhhl-j5i1ndgaM_yM/view?usp=sharing\n",
    "- https://drive.google.com/file/d/1709bhW6wcZx9jnypRFNPgnY-P51OEfIJ/view?usp=sharing\n",
    "\n",
    "You should not need a Google Account to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIPpEvI4kqMV"
   },
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The code below prepares our data, making several simplifications that are helpful when working with smaller datasets.\n",
    "\n",
    "#### Option (1) Run Code\n",
    "\n",
    "This can take a while sometimes, so if it does not finish within 5 minutes, use the json file instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "VYmEQgB7XoDE",
    "outputId": "05d8d89d-1dc4-41c7-d941-90bec8a0da06"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ted_en-20160408.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m targetXML \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mted_en-20160408.xml\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mUTF8\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# Get the contents of the <content> tag from the xml file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m target_text \u001b[39m=\u001b[39m etree\u001b[39m.\u001b[39mparse(targetXML)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ted_en-20160408.xml'"
     ]
    }
   ],
   "source": [
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "\n",
    "# Get the contents of the <content> tag from the xml file\n",
    "target_text = etree.parse(targetXML)\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# Remove \"Sound-effect labels\" using a regular expression (regex) (i.e. (Audio), (Laughter))\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# Break the text into sentences using the NLTK library\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "# Remove punctuation and change all characters to lowercase\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "     normalized_text.append(tokens)\n",
    "\n",
    "# Tokenise each sentence to process individual words\n",
    "sentences=[]\n",
    "sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
    "\n",
    "# Print a sample of 10 (tokenised) sentences\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZdHXgUXLaC6"
   },
   "source": [
    "#### Option (2) Use json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FT0V4WIfLaME",
    "outputId": "0b43d3c4-86c2-4e10-c0c6-380f8ee9b959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing'], ['consider', 'facit'], ['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them'], ['facit', 'was', 'a', 'fantastic', 'company'], ['they', 'were', 'born', 'deep', 'in', 'the', 'swedish', 'forest', 'and', 'they', 'made', 'the', 'best', 'mechanical', 'calculators', 'in', 'the', 'world'], ['everybody', 'used', 'them'], ['and', 'what', 'did', 'facit', 'do', 'when', 'the', 'electronic', 'calculator', 'came', 'along'], ['they', 'continued', 'doing', 'exactly', 'the', 'same']]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "sentences = json.load(open(\"ted_en-20160408.json\"))\n",
    "\n",
    "# Print a sample of 10 (tokenised) sentences\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CojV1MbhkQxK"
   },
   "source": [
    "### Word2Vec - Continuous Bag-Of-Words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLq1VIZ7TDog"
   },
   "source": [
    "For more details about gensim.models.word2vec you can refer to the [API for Gensim Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zW1iEee3lZC9"
   },
   "outputs": [],
   "source": [
    "# Initialize and train a word2vec model with the following parameters:\n",
    "# sentence: iterable of iterables, i.e. the list of lists of tokens from our data\n",
    "# size: dimensionality of the word vectors\n",
    "# window: window size\n",
    "# min_count: ignores all words with total frequency lower than the specified count value\n",
    "# workers: Use the specified number of worker threads to train the model (ie., enable faster training on multicore machines)\n",
    "# sg: training algorithm, 0 for CBOW, 1 for skip-gram\n",
    "wv_cbow_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FKp3X7pkRm6",
    "outputId": "055e9afc-e406-402d-cf70-732759a1aa57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8533000946044922),\n",
      " ('guy', 0.8113319277763367),\n",
      " ('lady', 0.7864135503768921),\n",
      " ('boy', 0.7814817428588867),\n",
      " ('girl', 0.7682338953018188),\n",
      " ('gentleman', 0.7450353503227234),\n",
      " ('soldier', 0.7398906946182251),\n",
      " ('kid', 0.7111135721206665),\n",
      " ('friend', 0.6803527474403381),\n",
      " ('photographer', 0.6772572994232178)]\n"
     ]
    }
   ],
   "source": [
    "# The trained word vectors are stored in a KeyedVectors instance as model.wv\n",
    "# Get the 10 most similar words to 'man' by calling most_similar() \n",
    "# most_similar() computes cosine similarity between a simple mean of the vectors of the given words and the vectors for each word in the model \n",
    "\n",
    "similar_words = wv_cbow_model.wv.most_similar(\"man\") # topn=10 by default\n",
    "pprint.pprint(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsFHg0znlPSf"
   },
   "source": [
    "### Word2Vec - Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "k16AowhCWUXu"
   },
   "outputs": [],
   "source": [
    "# Now we switch to a Skip Gram model by setting parameter sg=1\n",
    "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8UiVfr2cBtA",
    "outputId": "8b2d56b0-3e1c-4c8e-dfcd-f7982c303f14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.7846474647521973),\n",
      " ('rabbi', 0.7115237712860107),\n",
      " ('soldier', 0.7067371606826782),\n",
      " ('guy', 0.6972269415855408),\n",
      " ('boy', 0.6956959962844849),\n",
      " ('pianist', 0.6854783296585083),\n",
      " ('michelangelo', 0.6789897680282593),\n",
      " ('testament', 0.6779104471206665),\n",
      " ('dancer', 0.6662606596946716),\n",
      " ('gentleman', 0.6645552515983582)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = wv_sg_model.wv.most_similar(\"man\")\n",
    "pprint.pprint(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfF7YqvpppbG"
   },
   "source": [
    "## Word2Vec vs FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8IV7D6VAEcr"
   },
   "source": [
    "Word2Vec - Skip Gram cannot find similar words to \"electrofishing\" as \"electrofishing\" is not in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "oS9c2uWWquWG",
    "outputId": "000e8969-9a8a-46ca-f00b-4224a56937be"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0e3cbd324a01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimilar_words\u001b[0m  \u001b[0;34m=\u001b[0m\u001b[0mwv_sg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"electrofishing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Note: When run, this code should produce an error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'electrofishing' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "similar_words  =wv_sg_model.wv.most_similar(\"electrofishing\")\n",
    "pprint.pprint(similar_words)\n",
    "\n",
    "# Note: When run, this code should produce an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TpkScI8sA9G"
   },
   "source": [
    "### FastText - Skip Gram\n",
    "\n",
    "Now we will switch to FastText, which can handle out-of-vocabulary words (OOV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YAqOR1Vqps6M"
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kqkvyiUw_DRh"
   },
   "outputs": [],
   "source": [
    "# We initialize and train FastText with a Skip Gram architecture (sg=1)\n",
    "ft_sg_model = FastText(sentences, size=100, window=5, min_count=5, workers=2, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kv26QObJriB7",
    "outputId": "424d11f5-69bf-4490-c1c8-80fc9a6e0ab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('electrolux', 0.8027057647705078),\n",
      " ('electro', 0.7873678803443909),\n",
      " ('electrolyte', 0.7839568853378296),\n",
      " ('electric', 0.7741890549659729),\n",
      " ('airbus', 0.7718110084533691),\n",
      " ('electroshock', 0.7662405967712402),\n",
      " ('gastric', 0.7549958229064941),\n",
      " ('electrochemical', 0.754688560962677),\n",
      " ('electrogram', 0.7543359994888306),\n",
      " ('electrons', 0.7532363533973694)]\n"
     ]
    }
   ],
   "source": [
    "# Now, try 'electrofishing' and we will see that FastText allows us to obtain word vectors for out-of-vocabulary words\n",
    "result = ft_sg_model.wv.most_similar(\"electrofishing\")\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0x2aQpfsFSx"
   },
   "source": [
    "### FastText - Continuous Bag-Of-Words (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BUBqvqpc2sbL"
   },
   "outputs": [],
   "source": [
    "# Now we initialize and train FastText with CBOW architecture (sg=0)\n",
    "ft_cbow_model = FastText(sentences, size=100, window=5, min_count=5, workers=2, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kUj1RUzM2nLA",
    "outputId": "b8686ac3-d2df-4668-a1d9-090ad39e9db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('electric', 0.9155791997909546),\n",
      " ('electro', 0.9024492502212524),\n",
      " ('electrolux', 0.8950530290603638),\n",
      " ('electron', 0.8861533403396606),\n",
      " ('electronic', 0.8847971558570862),\n",
      " ('electrolyte', 0.8803984522819519),\n",
      " ('electroshock', 0.8720880746841431),\n",
      " ('electrode', 0.8711128234863281),\n",
      " ('electrical', 0.8702925443649292),\n",
      " ('electromagnet', 0.863209068775177)]\n"
     ]
    }
   ],
   "source": [
    "# Again, FastText allows us to obtain word vectors for out-of-vocabulary words\n",
    "result = ft_cbow_model.wv.most_similar(\"electrofishing\")\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hjmOhmRi7Ov"
   },
   "source": [
    "## King - Man + Woman = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw7b9OSwjGm0"
   },
   "source": [
    "Try using both CBOW and Skip Gram models to calculate \"King - Man + Woman = ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovTXjSdgrw36",
    "outputId": "e3cd2e5c-d38a-4241-e23f-bf9b5a47bfba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('president', 0.7592150568962097)]\n"
     ]
    }
   ],
   "source": [
    "# We can specify the positive/negative word list with the positive/negative parameters to create a word expression\n",
    "# Top N most similar words can be specified with the topn parameter\n",
    "result = wv_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUtbE2jwq1to",
    "outputId": "22feac29-4b4d-4c52-9027-2c77e0a208bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('luther', 0.6705532073974609)]\n"
     ]
    }
   ],
   "source": [
    "result = wv_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3PWf2I4_WZpG",
    "outputId": "2d42677c-7004-4ef8-cd19-0db00c652659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kidding', 0.89261394739151)]\n"
     ]
    }
   ],
   "source": [
    "result = ft_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9x51rRhWZrx",
    "outputId": "07e2cae2-72cf-408f-cf20-69e7ebbe662d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pauling', 0.6854696869850159)]\n"
     ]
    }
   ],
   "source": [
    "result = ft_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpAd8t-wjTMA"
   },
   "source": [
    "This is not what we expected...\n",
    "\n",
    "We are using relatively little data to form our embeddings. That means the representations are not going to be as good.\n",
    "\n",
    "In the next section, we will try again with a larger training dataset. Training ourselves would be too compute intensive, so we will use vectors Google trained using Google News data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMY5w8F7rElp"
   },
   "source": [
    "## Using Pretrained word embeddings with Gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keivkY13L4Nz"
   },
   "source": [
    "### 1.Download and load from Google pretrained Word2Vec binary file\n",
    "\n",
    "In case you are interested, here are links to the original code for word2vec:\n",
    "- [The Original Project](https://code.google.com/archive/p/word2vec/)\n",
    "- [GitHub Port of the original](https://github.com/tmikolov/word2vec)\n",
    "- [Another GitHub Repo](https://github.com/dav/word2vec) with an effort to update compatibility with other systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSWEK-vzCak8"
   },
   "source": [
    "#### Option (1) Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "teQvZDSirVVC"
   },
   "outputs": [],
   "source": [
    "# Download the pre-trained vectors trained on part of the Google News dataset (about 100 billion words)\n",
    "# Beware, this file is big (3.39GB) - you might need to wait a while! \n",
    "id2 = '0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n",
    "downloaded = drive.CreateFile({'id':id2}) \n",
    "downloaded.GetContentFile('GoogleNews-vectors-negative300.bin.gz')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffqbb9lGCbiW"
   },
   "source": [
    "#### Option (2) Local\n",
    "\n",
    "Download the file from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing and save it locally.\n",
    "\n",
    "You should not need a Google Account to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vy4C3llXCeMP"
   },
   "source": [
    "### Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "iTrXl4FRMitm"
   },
   "outputs": [],
   "source": [
    "# Decompress the downloaded file\n",
    "!gzip -d /content/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "64e_sRJ1rhUa"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Load the pretrained vectors with KeyedVectors instance\n",
    "# Note that we set the limit=100000 here, which means we set a maximum number of word-vectors to read from the file, to avoid out of memory issues and to load the vectors faster. \n",
    "\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "gn_wv_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PvMQp2-Tr3zl",
    "outputId": "a0e8296c-930f-44e9-c1ba-12b32dc5343f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071)]\n"
     ]
    }
   ],
   "source": [
    "# Now we can try to calculate \"King - Man + Woman = ?\" again\n",
    "result = gn_wv_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9N64tY4RNlw",
    "outputId": "76b3304c-7cfe-450f-b048-5aefad885775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5332662]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's also try to extract two example word embeddings and check their shape\n",
    "wv_banana = gn_wv_model[\"banana\"] \n",
    "wv_avocado = gn_wv_model[\"avocado\"]\n",
    "print(wv_banana.shape)\n",
    "print(wv_avocado.shape)\n",
    "\n",
    "# We can also calculate the cosine similarity ourselves with the extracted words\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([wv_banana],[wv_avocado])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12Ws7QvPMq9s"
   },
   "source": [
    "### 2.Load a pretrained word embedding model using the Gensim API\n",
    "The following code illustrates another way of loading pretrained word embeddings with Gensim. Here we try a GloVe embedding trained on Twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvAP4nyYM_qZ",
    "outputId": "ab4d480d-abf5-477a-eb16-3784c2c2d8cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
      "0.95908207\n",
      "0.040917932987213135\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# download the model and return it as an object ready for use\n",
    "model = api.load(\"glove-twitter-25\")  \n",
    "# The similarity() function can calculate the cosine similarity between two words\n",
    "print(model.similarity(\"cat\",\"dog\"))\n",
    "# The distance() function returns (1 - cosine similarity), which can be useful in some cases\n",
    "print(model.distance(\"cat\",\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqLruu6247Ze"
   },
   "source": [
    "# [Tips] Play with Colab Form Fields \n",
    "**The Form** supports multiple types of fields, including **input fields**, **dropdown menus**. \n",
    "\n",
    "In Lab2 E1, we already used the input fields. Let's try more now. You can edit this section by double-clicking it. \n",
    "\n",
    "Let's get familiar by changing the value in each input field (on the right) and checking the changes in the code (on the left) - and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBNvQmee5QIG",
    "outputId": "fbcbdb57-083e-4d6c-d5af-063b27c91395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string is examples\n",
      "slider_value 117\n"
     ]
    }
   ],
   "source": [
    "#@title Example form fields\n",
    "#@markdown please insert a description\n",
    "\n",
    "string = 'examples'  #@param {type: \"string\"}\n",
    "slider_value = 117  #@param {type: \"slider\", min: 100, max: 200}\n",
    "number = 102  #@param {type: \"number\"}\n",
    "date = '2020-01-05'  #@param {type: \"date\"}\n",
    "pick_me = \"monday\"  #@param ['monday', 'tuesday', 'wednesday', 'thursday']\n",
    "select_or_input = \"apples\" #@param [\"apples\", \"bananas\", \"oranges\"] {allow-input: true}\n",
    "\n",
    "\n",
    "#print the output\n",
    "print(\"string is\",string)\n",
    "print('slider_value',slider_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrjbuZYrXD88"
   },
   "source": [
    "# Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UWjBxLdTcEi"
   },
   "source": [
    "## Word Embedding Visual Inspector (WEVI)\n",
    "If you would like to visualise how Word2Vec is learning, the following link is useful https://ronxin.github.io/wevi/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec1f34e7810c2f5eb988dadaa716a28bb15ba5f1094ff42ffd46ca9b187ffa6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
